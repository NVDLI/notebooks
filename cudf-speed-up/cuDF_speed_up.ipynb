{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "cuDF_speed_up.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XMDuUoUl5cQ"
      },
      "source": [
        "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded/images/EDU/DLI%20Asset%20-%20Logo.jpg\" width=\"400\" height=\"186\" /></a></center>"
      ],
      "id": "_XMDuUoUl5cQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1WbsSK_0Xqo"
      },
      "source": [
        "# Speed Up DataFrame Operations w/ RAPIDS cuDF"
      ],
      "id": "T1WbsSK_0Xqo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bPAvj4fwjbq"
      },
      "source": [
        "## Welcome\n",
        "A **DataFrame** is a 2-dimensional data structure used to represent data in a tabular format, like a spreadsheet or SQL table. Originally offered through the Python Data Analysis ([pandas](https://pandas.pydata.org/docs/)) library, DataFrames have become very popular for its familiar representation along with a robust set of features that are intuitive and expressive. \n",
        "\n",
        "Raw data often needs to be manipulated before it can be used for further purposes such as generating **Business Intelligence**, creating **Dashboard Visualization**, or training **Machine Learning** models. These preprocessing steps can include **filtering**, **merging**, **grouping**, and **aggregating**. \n",
        "\n",
        "Below is a typical data processing pipeline: \n",
        "![flow](https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/flow.png?raw=true)\n",
        "\n",
        "According to [studies](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/?sh=29f71b266f63), data preparation accounts for ~80% of the work for analysts. This could be due in part to the rapid increase in the size of data as well as the iterative nature of analytics. \n",
        "\n",
        "Recognizing this potential bottleneck, NVIDIA created [**cuDF**](https://docs.rapids.ai/api/cudf/stable/) that leverages GPU hardware and software to perform data manipulation tasks with parallel computing, **saving valuable time and resources**. The cuDF library is part of the larger [**RAPIDS**](https://rapids.ai/) data science framework that allows for the execution of **end-to-end analytics pipelines** entirely on GPUs. One of the focus for cuDF and its companion suite of open source software libraries is to provide syntax that is similar to their CPU counterparts, **making it easy to implement**. \n",
        "\n",
        "This notebook is intended to demonstrate speedup in data processing by moving common DataFrame operations to the GPU with minimal changes to existing code. "
      ],
      "id": "-bPAvj4fwjbq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ComTzf6gEWwT"
      },
      "source": [
        "### Environment Sanity Check\n",
        "\n",
        "Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n",
        "\n",
        "Check the output of `!nvidia-smi` to make sure you've been allocated a RAPIDS supported GPU such as Tesla T4, P4, or P100."
      ],
      "id": "ComTzf6gEWwT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c58af14d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "c58af14d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4yzQAnxEdOF"
      },
      "source": [
        "### Setup\n",
        "Because RAPIDS cuDF isn't readily available in this Google Colab environment, it needs to be installed following the below steps: \n",
        "1. Updates gcc in Colab\n",
        "2. Installs Conda\n",
        "3. Install RAPIDS' current stable version of its libraries\n",
        "4. Copy RAPIDS' .so files into current working directory, a neccessary workaround for RAPIDS+Colab integration.\n"
      ],
      "id": "A4yzQAnxEdOF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XY491dEEMqF"
      },
      "source": [
        "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
        "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/env-check.py"
      ],
      "id": "3XY491dEEMqF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_79HZuYtxKL"
      },
      "source": [
        "![pause](https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/pause.png?raw=true)\n",
        "\n",
        "Were you assigned a compatible GPU? \n",
        "If not, click the _Runtime_ dropdown at the top of the page, then _Factory Reset Runtime_ to get another assignment."
      ],
      "id": "w_79HZuYtxKL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KalRo0xsEoyb"
      },
      "source": [
        "# This will update the Colab environment and restart the kernel.  Don't run the next cell until you see the session crash.\n",
        "!bash rapidsai-csp-utils/colab/update_gcc.sh\n",
        "import os\n",
        "os._exit(00)"
      ],
      "id": "KalRo0xsEoyb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6l3EE5SyRYY"
      },
      "source": [
        "![pause](https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/pause.png?raw=true)\n",
        "\n",
        "Don't run the next cell until Colab session has crashed. "
      ],
      "id": "t6l3EE5SyRYY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qE98bujEqRW"
      },
      "source": [
        "# This will install CondaColab.  This will restart your kernel one last time.  Run this cell by itself and only run the next cell once you see the session crash.\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "id": "9qE98bujEqRW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q7ubxe1TpHt"
      },
      "source": [
        "![pause](https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/pause.png?raw=true)\n",
        "\n",
        "Don't run the next cell until Colab session has crashed. "
      ],
      "id": "0Q7ubxe1TpHt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CohiENWHFTfo"
      },
      "source": [
        "# you can now run the rest of the cells as normal\n",
        "import condacolab\n",
        "condacolab.check()"
      ],
      "id": "CohiENWHFTfo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI-sGJUtyfQb"
      },
      "source": [
        "Next we can begin the installation of RAPIDS in this Colab environment. This step can take up to ~15 mins. Execute the _next cell_ and check out this article on [**10 Minutes to cdDF and Dask-cuDF**](https://docs.rapids.ai/api/cudf/stable/10min.html) while the installation completes. "
      ],
      "id": "hI-sGJUtyfQb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnjJuPU9FYN5"
      },
      "source": [
        "# Installing RAPIDS is now 'python rapidsai-csp-utils/colab/install_rapids.py <release> <packages>'\n",
        "!python rapidsai-csp-utils/colab/install_rapids.py stable core\n",
        "import os\n",
        "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
        "os.environ['CONDA_PREFIX'] = '/usr/local'"
      ],
      "id": "RnjJuPU9FYN5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM2FQ7-P8iaF"
      },
      "source": [
        "## Interactive Exercise"
      ],
      "id": "GM2FQ7-P8iaF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKUJgAqC38jR"
      },
      "source": [
        "import numpy as np # for generating sample data\n",
        "\n",
        "import pandas as df\n",
        "# import cudf as df\n",
        "import time # for clocking process times\n",
        "import matplotlib.pyplot as plt # for visualizing results\n",
        "\n",
        "class Timer: # creating a Timer helper class to measure execution time\n",
        "  def __enter__(self):\n",
        "    self.start=time.perf_counter()\n",
        "    return self\n",
        "  def __exit__(self, *args):\n",
        "    self.end=time.perf_counter()\n",
        "    self.interval=self.end-self.start"
      ],
      "id": "XKUJgAqC38jR",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjeW2Mdh0huU"
      },
      "source": [
        "### Loading a Sample Data\n",
        "We start our demonstration by generating two 2-dimensional arrays of random numbers - we've configured for sizeable arrays at 1MM rows by 50 columns each. Then they are converted to DataFrames using ```pandas.DataFrame()``` or ```cudf.DataFrame()```:"
      ],
      "id": "GjeW2Mdh0huU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSCUQYModrAd"
      },
      "source": [
        "rows=1000000\n",
        "columns=50"
      ],
      "id": "RSCUQYModrAd",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "108eb7cb"
      },
      "source": [
        "def load_data(): \n",
        "  data_a=np.random.randint(0, 100, (rows, columns))\n",
        "  data_b=np.random.randint(0, 100, (rows, columns))\n",
        "  dataframe_a=df.DataFrame(data_a, columns=[f'a_{i}' for i in range(columns)])\n",
        "  dataframe_b=df.DataFrame(data_b, columns=[f'b_{i}' for i in range(columns)])\n",
        "  return dataframe_a, dataframe_b\n",
        "\n",
        "with Timer() as process_time: \n",
        "  dataframe_a, dataframe_b=load_data()\n",
        "\n",
        "print(f'The loading process took {process_time.interval:.2f} seconds')\n",
        "display(dataframe_a.tail(5))\n",
        "display(dataframe_b.tail(5))"
      ],
      "id": "108eb7cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXlraNW9cl31"
      },
      "source": [
        "![check](https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/check.png?raw=true)\n",
        "\n",
        "We created two DataFrames, _dataframe_a_ and _dataframe_b_ that are 1000000 rows by 50 columns (col_1, col_2, ... col_48, col_49) each. "
      ],
      "id": "sXlraNW9cl31"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKYzyh6bxwAB"
      },
      "source": [
        "### Merging Data\n",
        "Sometimes data can come from multiple sources and need to be merged into one with ```DataFrame.merge()```. For example, a typical retail data storage infrastructure may include a customer table and separate transaction and product tables. Merging the data allows the correct details to be included in a single DataFrame to get the insight needed. "
      ],
      "id": "DKYzyh6bxwAB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAGSwY8qx2DB"
      },
      "source": [
        "def merge_data(left_df, right_df):\n",
        "  combined_df=df.merge(left_df, right_df, left_index=True, right_index=True)\n",
        "  return combined_df\n",
        "\n",
        "with Timer() as process_time: \n",
        "  combined_df=merge_data(dataframe_a, dataframe_b)\n",
        "\n",
        "print(f'The merging process took {process_time.interval:.2f} seconds')\n",
        "display(combined_df.head())"
      ],
      "id": "bAGSwY8qx2DB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_1QcS17c3S5"
      },
      "source": [
        "![check](https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/check.png?raw=true)\n",
        "\n",
        "We merged two DataFrames, _dataframe_a_ and _dataframe_b_ on their _index_ into one larger DataFrame that is 1000000 rows by 100 columns (a_0, a_1, ..., b_48, b_49). "
      ],
      "id": "S_1QcS17c3S5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhdsvT-gABvZ"
      },
      "source": [
        "### Summarize\n",
        "Exploring data begins with **descriptive statistics**, which often involves finding the **central tendency** and **dispersion**. They are a quick way to summarize distributions. Measures of central tendency includes the mean, median, and mode - they are used to describe the center of a set of data values. Measures of dispersion include variance and standard deviation - they are used to describe the degree to which data is distributed around the center. We can quickly perform simple descriptive statistics with the ```DataFrame.describe()``` method. "
      ],
      "id": "UhdsvT-gABvZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26a2c5b6"
      },
      "source": [
        "def summarize(dataframe):\n",
        "  summary_df=dataframe.describe()\n",
        "  return summary_df\n",
        "\n",
        "with Timer() as process_time: \n",
        "  summary_df=summarize(combined_df)\n",
        "\n",
        "print(f'The summarizing process took {process_time.interval:.2f} seconds')\n",
        "display(summary_df)"
      ],
      "id": "26a2c5b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPz54wMldInX"
      },
      "source": [
        "![check](https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/check.png?raw=true)\n",
        "\n",
        "Since this is a sample data set, we see that each of columns/features (a_0, a_1, ..., b_48, b_49) have 1000000 values with an average ~50 and standard deviation of ~30"
      ],
      "id": "KPz54wMldInX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7N64bdRAclS"
      },
      "source": [
        "### Correlation - Exploring Relationships\n",
        "We might be interested in finding relationships/dependencies between two or more variables through their correlation with ```DataFrame.corr()```. Correlation is a number between -1 and 1 that describes the strength of the association between two variables. Two variables with a correlation of 1 suggests that they change together in the same direction while a correlation of -1 suggests that they change together in the opposite direction. "
      ],
      "id": "w7N64bdRAclS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2538ccdd"
      },
      "source": [
        "def correlation(dataframe): \n",
        "  corr_df=dataframe.corr()\n",
        "  return corr_df\n",
        "\n",
        "with Timer() as process_time: \n",
        "  corr_df=correlation(combined_df)\n",
        "\n",
        "print(f'The correlation process took {process_time.interval:.2f} seconds')\n",
        "display(corr_df.head())"
      ],
      "id": "2538ccdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaiK9t2CdgFS"
      },
      "source": [
        "![check](https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/check.png?raw=true)\n",
        "\n",
        "The resulting cross tabulation shows that each column/feature (a_0, a_1, ..., b_48, b_49) have a perfect correlation (1) with itself and is not correlated (~0) with each other. "
      ],
      "id": "uaiK9t2CdgFS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j1Y3Y_kBYyY"
      },
      "source": [
        "### Grouping\n",
        "We can compare subsets of the data to explore the significance of categories and classes with the ```DataFrame.groupby()``` method. We can even group continuous data values into a smaller number of bins with ```pandas.cut()``` or ```cudf.cut()``` to simplify our analysis. The groupings usually follow an aggregation such as mean or count. For example, we can group our data into 5 equidistant bins based on their sequential index. "
      ],
      "id": "1j1Y3Y_kBYyY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d050021a"
      },
      "source": [
        "def groupby_summarize(dataframe):\n",
        "  dataframe['group']=dataframe.index\n",
        "  dataframe['group']=df.cut(dataframe['group'], 5)\n",
        "  group_describe_df=dataframe.groupby('group').mean().reset_index(drop=True)\n",
        "  return group_describe_df\n",
        "\n",
        "with Timer() as process_time: \n",
        "  group_describe_df=groupby_summarize(combined_df)\n",
        "\n",
        "print(f'The grouping process took {process_time.interval:.2f} seconds')\n",
        "display(group_describe_df)"
      ],
      "id": "d050021a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdVGBVr9e_o8"
      },
      "source": [
        "![check](https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/check.png?raw=true)\n",
        "\n",
        "The resulting DataFrame shows that each group maintains an average of ~50 for each column/feature (a_0, a_1, ..., b_48, b_49) as expected for this sample data. "
      ],
      "id": "LdVGBVr9e_o8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-9gbIriKa85"
      },
      "source": [
        "### Putting it together\n",
        "We can measure the total elapsed time for this sample data processing workflow. "
      ],
      "id": "b-9gbIriKa85"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMLKNN_RPB0c"
      },
      "source": [
        "def pipeline():\n",
        "  performance={}\n",
        "\n",
        "  with Timer() as process_time: \n",
        "    dataframe_a, dataframe_b=load_data()\n",
        "  performance['load data']=process_time.interval\n",
        "\n",
        "  with Timer() as process_time: \n",
        "    combined_df=merge_data(dataframe_a, dataframe_b)\n",
        "  performance['merge data']=process_time.interval\n",
        "  \n",
        "  with Timer() as process_time: \n",
        "    summarize(combined_df)\n",
        "  performance['summarize']=process_time.interval\n",
        "  \n",
        "  with Timer() as process_time: \n",
        "    correlation(combined_df)\n",
        "  performance['correlation']=process_time.interval\n",
        "  \n",
        "  with Timer() as process_time: \n",
        "    groupby_summarize(combined_df)\n",
        "  performance['groupby & summarize']=process_time.interval\n",
        "  \n",
        "  if df.__name__=='cudf': \n",
        "    df.DataFrame([performance], index=['gpu']).to_pandas().plot(kind='bar', stacked=True)\n",
        "  else: \n",
        "    df.DataFrame([performance], index=['gpu']).plot(kind='bar', stacked=True)\n",
        "\n",
        "  return None"
      ],
      "id": "HMLKNN_RPB0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csfRLkjsc2v8"
      },
      "source": [
        "### Timing the Pipeline on CPU"
      ],
      "id": "csfRLkjsc2v8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DcmBph9cyjm"
      },
      "source": [
        "import pandas as df\n",
        "pipeline()"
      ],
      "id": "8DcmBph9cyjm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2nKfQWD7V1k"
      },
      "source": [
        "### Switching to GPU\n",
        "Traditionally, these tasks are frequently done (as we did) using the popular [**pandas**](https://pandas.pydata.org/) library, which only runs on a single CPU. NVIDIA's [**cuDF**](https://docs.rapids.ai/api/cudf/stable/) library was built with the users in mind - by offering nearly identical syntax to its CPU counterpart, developers only have to make few changes to their existing code to take advantage of its capabilities. "
      ],
      "id": "T2nKfQWD7V1k"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfDvbYbIU4b1"
      },
      "source": [
        "import cudf as df"
      ],
      "id": "TfDvbYbIU4b1",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeYOoMVOLIbD"
      },
      "source": [
        "**That's it!** cuDF uses nearly identical syntax to the familiar pandas API. **Brilliant!** It's worth noting that there are some features that are unique to each library, but conviniently there are a lot of overlaps. "
      ],
      "id": "oeYOoMVOLIbD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocdd-JmXK5gg"
      },
      "source": [
        "pipeline()"
      ],
      "id": "Ocdd-JmXK5gg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgU3PNaPLZsS"
      },
      "source": [
        "### Comparing Results\n",
        "In a trial run, **cuDF** completed the data processing tasks in nearly 10x faster than **pandas**. The expectations is that the speedup will be even more significant as the size of the data becomes largers. Feel free to give it a try by modifying the dimensions of the data above. \n",
        "\n",
        "![result](https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/result.png?raw=true)"
      ],
      "id": "cgU3PNaPLZsS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYPyye6BYNbr"
      },
      "source": [
        "## Conclusion\n",
        "Congratulations on completing the notebook! Want to learn more about cuDF and the rest of the RAPIDS framework? Check out the follow-up to this course, [Accelerating End-to-End Data Science Workflows]('https://courses.nvidia.com/courses/course-v1:DLI+S-DS-01+V1/about') or our other online courses at [NVIDIA DLI]('https://www.nvidia.com/en-us/training/online/')."
      ],
      "id": "lYPyye6BYNbr"
    }
  ]
}